import streamlit as st
import numpy as np
import requests
from transformers import BertTokenizerFast
import torch
import os

st.set_page_config(page_title="NER Project Demo (Triton)", page_icon="üß†", layout="centered")

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
TRITON_URL = "http://localhost:8000/v2/models/bert_ner/infer"
MODEL_NAME = "bert_ner"  # –ò–º—è –º–æ–¥–µ–ª–∏ –≤ Triton
TOKENIZER_NAME = "bert-base-cased" # –ò–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
TAG2IDX_PATH = "models/tag2idx.pt" # –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é —Ç–µ–≥–æ–≤

LABEL_MAPPING = {
    "per": "Person",
    "geo": "Location",
    "gpe": "Location",
    "org": "Organization",
    "tim": "Time",
    "art": "Artifact",
    "eve": "Event",
    "nat": "Nature"
}

COLOR_MAP = {
    "geo": "#bae6fd", "gpe": "#bae6fd",
    "per": "#fecaca",
    "org": "#bbf7d0",
    "tim": "#fef08a",
    "art": "#e9d5ff", "eve": "#fed7aa",
    "nat": "#e5e7eb"
}

@st.cache_resource
def load_resources():
    try:
        tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_NAME)
        
        if os.path.exists(TAG2IDX_PATH):
            tag2idx = torch.load(TAG2IDX_PATH)
            idx2tag = {v: k for k, v in tag2idx.items()}
        else:
            st.error(f"–§–∞–π–ª —Å–ª–æ–≤–∞—Ä—è {TAG2IDX_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return None, None

        return tokenizer, idx2tag
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤: {e}")
        return None, None

tokenizer, idx2tag = load_resources()

def query_triton(text, tokenizer):
    inputs = tokenizer(
        text,
        return_tensors="np",
        padding="max_length",
        truncation=True,
        max_length=128,
        return_offsets_mapping=True
    )
    
    input_ids = inputs["input_ids"].astype(np.int64)
    attention_mask = inputs["attention_mask"].astype(np.int64)
    offset_mapping = inputs["offset_mapping"][0] # [Seq, 2]

    payload = {
        "inputs": [
            {
                "name": "input_ids",
                "shape": input_ids.shape,
                "datatype": "INT64",
                "data": input_ids.tolist()
            },
            {
                "name": "attention_mask",
                "shape": attention_mask.shape,
                "datatype": "INT64",
                "data": attention_mask.tolist()
            }
        ],
        "outputs": [
            {
                "name": "logits"
            }
        ]
    }

    try:
        response = requests.post(TRITON_URL, json=payload)
        response.raise_for_status()
        result_data = response.json()
        
        # –§–æ—Ä–º–∞—Ç Triton JSON response: {"outputs": [{"name": "logits", "data": [...], "shape": [...]}]}
        logits_data = result_data["outputs"][0]["data"]
        shape = result_data["outputs"][0]["shape"]
        
        logits = np.array(logits_data).reshape(shape)
        preds = np.argmax(logits, axis=2)[0] # [Seq]
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ Triton Inference: {e}")
        return []

    entities = []
    current_entity = None

    for idx, (pred_idx, offset) in enumerate(zip(preds, offset_mapping)):
        start, end = offset
        if start == end: continue
        
        tag = idx2tag.get(pred_idx, "O")
        
        if tag.startswith("B-"):
            if current_entity:
                entities.append(current_entity)
            current_entity = {
                "entity_group": tag.split("-")[1],
                "start": int(start),
                "end": int(end),
                "score": 1.0
            }
        elif tag.startswith("I-") and current_entity:
            type_ = tag.split("-")[1]
            if type_ == current_entity["entity_group"]:
                current_entity["end"] = int(end)
            else:
                entities.append(current_entity)
                current_entity = None
        else:
            if current_entity:
                entities.append(current_entity)
                current_entity = None
                
    if current_entity:
        entities.append(current_entity)
        
    return entities

def render_ner_html(text, entities):
    html_content = '<div style="line-height: 3.5; font-family: sans-serif; font-size: 16px; margin-bottom: 3rem;">'
    last_idx = 0
    
    entities = sorted(entities, key=lambda x: x['start'])
    
    for entity in entities:
        start, end = entity['start'], entity['end']
        raw_label = entity['entity_group']
        word = text[start:end]
        
        readable_label = LABEL_MAPPING.get(raw_label.lower(), raw_label.upper())
        color = COLOR_MAP.get(raw_label, "#e5e7eb")
        
        if start > last_idx:
            html_content += f'<span>{text[last_idx:start]}</span>'
        
        entity_html = f"""
        <span style="display: inline-block; position: relative; line-height: 1.0; vertical-align: baseline; margin: 0 4px;">
            <span style="
                background-color: {color}; 
                color: #111827; 
                padding: 4px 6px; 
                border-radius: 6px; 
                font-weight: 500;
                border: 1px solid rgba(0,0,0,0.1);">
                {word}
            </span>
            <span style="
                position: absolute;
                top: 100%;
                left: 50%;
                transform: translateX(-50%);
                font-size: 0.75em;
                color: {color}; 
                margin-top: 0.5rem;
                font-weight: 600;
                white-space: nowrap;
                pointer-events: none;
                opacity: 0.9;">
                {readable_label}
            </span>
        </span>
        """
        html_content += entity_html
        last_idx = end
        
    if last_idx < len(text):
        html_content += f'<span>{text[last_idx:]}</span>'
        
    html_content += '</div>'
    return html_content

st.title("üîç NER: –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (Triton Inference)")
st.markdown(f"–°–µ—Ä–≤–µ—Ä: `{TRITON_URL}` | –ú–æ–¥–µ–ª—å: `{MODEL_NAME}`")

default_text = "Steve Jobs presented the new iPhone in San Francisco at the Apple headquarters."
text = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç:", default_text, height=100)

if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", type="primary"):
    if tokenizer and text:
        with st.spinner("–ó–∞–ø—Ä–æ—Å –∫ Triton Server..."):
            results = query_triton(text, tokenizer)
            html_result = render_ner_html(text, results)
            
            st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            st.markdown(html_result, unsafe_allow_html=True)
            st.write("")
            
            with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (JSON)"):
                st.json(results)
    else:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (Triton/Tokenizer). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏.")

