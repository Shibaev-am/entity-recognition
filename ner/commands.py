import os
import subprocess
import sys
from pathlib import Path

import typer
from typing_extensions import Annotated

app = typer.Typer(name="ner", help="BERT NER MLOps Project")

PROJECT_ROOT = Path(__file__).parent.parent


@app.command()
def train(
    overrides: list[str] = typer.Argument(
        None, help="Hydra overrides, –Ω–∞–ø—Ä–∏–º–µ—Ä: trainer.max_epochs=5"
    ),
):
    from scripts.train import train as train_fn

    sys.argv = ["train"]
    if overrides:
        sys.argv.extend(overrides)
    train_fn()


@app.command("to-onnx")
def to_onnx(
    overrides: list[str] = typer.Argument(
        None, help="Hydra overrides, –Ω–∞–ø—Ä–∏–º–µ—Ä: paths.model_save_dir=./other_models"
    ),
):
    from scripts.to_onnx import convert_to_onnx

    sys.argv = ["to_onnx"]
    if overrides:
        sys.argv.extend(overrides)
    convert_to_onnx()


@app.command()
def infer(
    overrides: list[str] = typer.Argument(None, help="Hydra overrides"),
):
    from scripts.infer import infer as infer_fn

    sys.argv = ["infer"]
    if overrides:
        sys.argv.extend(overrides)
    infer_fn()


@app.command("to-tensorrt")
def to_tensorrt():
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ONNX –º–æ–¥–µ–ª–∏ –≤ TensorRT engine."""
    script_path = PROJECT_ROOT / "scripts" / "to_tensorrt.sh"

    if not script_path.exists():
        typer.echo(f"‚ùå –°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {script_path}")
        raise typer.Exit(1)

    result = subprocess.run(["bash", str(script_path)], cwd=PROJECT_ROOT)

    if result.returncode != 0:
        typer.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ TensorRT")
        raise typer.Exit(result.returncode)

    typer.echo("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ TensorRT –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


@app.command("demo-local")
def demo_local():
    demo_path = PROJECT_ROOT / "apps" / "local_run.py"
    subprocess.run(["streamlit", "run", str(demo_path)], check=True)


@app.command("run-triton")
def run_triton(
    device: Annotated[
        str,
        typer.Option(help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –∏–ª–∏ CPU"),
    ] = "gpu",
    backend: Annotated[
        str,
        typer.Option(help="–¢–∏–ø –º–æ–¥–µ–ª–∏: 'onnx' –∏–ª–∏ 'tensorrt'"),
    ] = "onnx",
):
    """–ó–∞–ø—É—Å–∫ Triton Inference Server —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é."""
    # print(device, backend)
    # exit()
    backend = backend.lower()
    if backend not in ("onnx", "tensorrt"):
        typer.echo(
            f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {backend}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'onnx' –∏–ª–∏ 'tensorrt'"
        )
        raise typer.Exit(1)

    gpu_mode = "gpu" if device == "gpu" else "no-gpu"
    script_path = PROJECT_ROOT / "scripts" / "run_triton_server.sh"

    if not script_path.exists():
        typer.echo(f"‚ùå –°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {script_path}")
        raise typer.Exit(1)

    result = subprocess.run(
        ["bash", str(script_path), gpu_mode, backend],
        cwd=PROJECT_ROOT,
    )

    if result.returncode != 0:
        raise typer.Exit(result.returncode)


@app.command("run-app")
def demo_triton(
    backend: str = typer.Argument(
        "onnx",
        help="–ë—ç–∫–µ–Ω–¥ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: 'onnx' –∏–ª–∏ 'tensorrt'",
    )
):
    if backend not in ("onnx", "tensorrt"):
        typer.echo(
            f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—ç–∫–µ–Ω–¥: {backend}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'onnx' –∏–ª–∏ 'tensorrt'"
        )
        raise typer.Exit(1)

    # –ü–µ—Ä–µ–¥–∞—ë–º backend –≤ Streamlit —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
    env = os.environ.copy()
    env["TRITON_BACKEND"] = backend

    demo_path = PROJECT_ROOT / "apps" / "triton_run.py"
    typer.echo(f"üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ —Å –±—ç–∫–µ–Ω–¥–æ–º: {backend.upper()}")
    subprocess.run(["streamlit", "run", str(demo_path)], env=env, check=True)

    # demo_path = PROJECT_ROOT / "apps" / "triton_run.py"
    # subprocess.run(["streamlit", "run", str(demo_path)], check=True)


def main():
    app()


if __name__ == "__main__":
    main()
